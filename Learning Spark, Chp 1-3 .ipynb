{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "- [Charles Bochet - Get Started with PySpark and Jupyter Notebook in 3 Minutes](https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f)\n",
    "- [Karau, Holden, Andy Konwinski, Patrick Wendell, and Matei Zaharia. Learning spark: lightning-fast big data analysis. \" O'Reilly Media, Inc.\", 2015.](https://proquest-safaribooksonline-com.proxy.library.cmu.edu/book/databases/business-intelligence/9781449359034/preface/idp4948496_html#X2ludGVybmFsX0h0bWxWaWV3P3htbGlkPTk3ODE0NDkzNTkwMzQlMkZjaGFwX3BhaXJfcmRkc19odG1sJnF1ZXJ5PQ==)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T19:26:08.494453Z",
     "start_time": "2019-02-06T19:26:08.487279Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T19:26:09.174060Z",
     "start_time": "2019-02-06T19:26:09.152611Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='my')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above code adds PySpark into sys.path at runtime and creates a Spark context for later use**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T19:26:57.608815Z",
     "start_time": "2019-02-06T19:26:57.527424Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile('data/README.md') # the README file from findspark project page on Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T19:26:58.660831Z",
     "start_time": "2019-02-06T19:26:58.544169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T19:26:59.849954Z",
     "start_time": "2019-02-06T19:26:59.779829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Find spark'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T19:27:00.600080Z",
     "start_time": "2019-02-06T19:27:00.592947Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pythonlines = lines.filter(lambda line: \"Python\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T19:27:02.627278Z",
     "start_time": "2019-02-06T19:27:02.517642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Findspark can add a startup file to the current IPython profile so that the environment vaiables will be properly set and pyspark will be imported upon IPython startup. This file is created when `edit_profile` is set to true.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonlines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Transformations and actions are different because of the way Spark computes RDDs. Although you can define new RDDs any time, Spark computes them only in a lazy fashion—that is, the first time they are used in an action. This approach might seem unusual at first, but makes a lot of sense when you are working with Big Data. For instance, consider Example 3-2 and Example 3-3, where we defined a text file and then filtered the lines that include Python. If Spark were to load and store all the lines in the file as soon as we wrote lines = sc.textFile(...), it would waste a lot of storage space, given that we then immediately filter out many lines. Instead, once Spark sees the whole chain of transformations, it can compute just the data needed for its result. In fact, for the first() action, Spark scans the file only until it finds the first matching line; it doesn’t even read the whole file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T19:27:05.269052Z",
     "start_time": "2019-02-06T19:27:05.252206Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pythonlines = lines.filter(lambda line: \"Python\" in line).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T19:27:06.137244Z",
     "start_time": "2019-02-06T19:27:05.969240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Findspark can add a startup file to the current IPython profile so that the environment vaiables will be properly set and pyspark will be imported upon IPython startup. This file is created when `edit_profile` is set to true.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonlines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Finally, Spark’s RDDs are by default recomputed each time you run an action on them. If you would like to reuse an RDD in multiple actions, you can ask Spark to persist it using RDD.persist(). In practice, you will often use persist() to load a subset of your data into memory and query it repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two ways to create an RDD\n",
    "- loading external files\n",
    "    - like in the previous e.g.\n",
    "- parallelizing an existing dataset\n",
    "    - only used for prototyping since this requires the dataset to be in the memory of one machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T19:16:33.510514Z",
     "start_time": "2019-02-06T19:16:33.306411Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "lines = sc.parallelize([\"pandas\", \"i like pandas\"])\n",
    "lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations v.s. Actions\n",
    "- transformations return RDDs and are exectued lazily\n",
    "    - filter()\n",
    "    - union(), distinct(), intersetion(), suctract(), cartesian(), ...\n",
    "    - map(), flatMap()\n",
    "- actions return other types of data and kick off computations\n",
    "    - count()\n",
    "    - take()\n",
    "    - reduce(), fold()\n",
    "    - aggregate()\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the set operations, they are generally _very_ expensive except $\\text{union}$, since they require Shuffling data across the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One issue to watch out for when passing functions is inadvertently serializing the object containing the function. When you pass a function that is the member of an object, or contains references to fields in an object (e.g., self.field), Spark sends the entire object to worker nodes, which can be much larger than the bit of information you need (see Example 3-19). Sometimes this can also cause your program to fail, if your class contains objects that Python can’t figure out how to pickle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is a good example of avoiding passing functions that have field references\n",
    "```python\n",
    "class WordFunctions(object):\n",
    "  ...\n",
    "  def getMatchesNoReference(self, rdd):\n",
    "      # Safe: extract only the field we need into a local variable\n",
    "      query = self.query\n",
    "      return rdd.filter(lambda x: query in x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T19:29:33.595620Z",
     "start_time": "2019-02-06T19:29:33.533545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Find spark\n",
      "\n",
      "PySpark isn't on sys.path by default, but that doesn't mean it can't be used as a regular library.\n",
      "You can address this by either symlinking pyspark into your site-packages,\n",
      "or adding pyspark to sys.path at runtime. `findspark` does the latter.\n",
      "\n",
      "To initialize PySpark, just call\n",
      "\n",
      "```python\n",
      "import findspark\n"
     ]
    }
   ],
   "source": [
    "for line in lines.take(10):\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T19:39:50.745149Z",
     "start_time": "2019-02-06T19:39:50.660263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n",
      "36\n",
      "49\n",
      "64\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize(list(range(100)))\n",
    "squared = nums.map(lambda x: x*x)\n",
    "for nn in squared.take(10):\n",
    "    print('{}'.format(nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T19:38:34.209118Z",
     "start_time": "2019-02-06T19:38:34.079791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "world\n",
      "hello\n",
      "spark\n",
      "hello\n",
      "allen\n"
     ]
    }
   ],
   "source": [
    "lines = sc.parallelize(['hello world', 'hello spark', 'hello allen'])\n",
    "words = lines.flatMap(lambda pp: pp.split(' ')).collect() # use collect to trigger an action\n",
    "for word in words:\n",
    "    print(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T19:49:25.808016Z",
     "start_time": "2019-02-06T19:49:25.674055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hello world', 0)\n",
      "('hello world', 1)\n",
      "('hello world', 2)\n",
      "('hello world', 3)\n",
      "('hello world', 4)\n",
      "('hello world', 5)\n",
      "('hello world', 6)\n",
      "('hello world', 7)\n",
      "('hello world', 8)\n",
      "('hello world', 9)\n"
     ]
    }
   ],
   "source": [
    "for el in lines.cartesian(nums).take(10):\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T19:56:25.476279Z",
     "start_time": "2019-02-06T19:56:25.368865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.5\n"
     ]
    }
   ],
   "source": [
    "sumCount = nums.aggregate((0, 0), # the accumulator\n",
    "                          (lambda acc, value: (acc[0] + value, acc[1] + 1)), # how to combine the RDD with the accumulator\n",
    "                          (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))) # how to combind res. from two acc.\n",
    "print(sumCount[0] / float(sumCount[1])) # the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T20:00:26.454482Z",
     "start_time": "2019-02-06T20:00:26.338666Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.5"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In Scala and Java, these type of methods are only defined for specific types of RDDs.**\n",
    "e.g., mean() and variance() for numeric RDDs, and join() for key-value pair RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
