{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T22:19:59.324285Z",
     "start_time": "2019-03-08T22:19:59.320159Z"
    }
   },
   "source": [
    "## What is Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral clustering is one of the most popular modern clustering algorithms. It builds a similarity matrix from the data, construct a graph from this matrix, and then clusters the observations based on the spectral properties of the graph Laplacian. The basic idea is simple to implement and yet it has a wide range of application and often out performs other more traditional algorithms like k-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Spectral Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are many different versions or variations of spectral clustering algorithm, the most common one by von Luxburg: can be described as below:\n",
    "\n",
    "1. Construct a similarity matrix $S$, with each entry $S_{ij}$ measuring the similarity of sample $i$ and $j$.\n",
    "2. Construct a similarity graph from $S$, and retrieve the adjacency matrix $W$.\n",
    "3. With $D$ as the diagonal matrix of node degrees, define graph Laplacian as either\n",
    "    - $L = D - W$, or\n",
    "    - $L_{IW} = I - D^{-1}W$, or\n",
    "    - $L_{sym} = I - D^{-1/2}WD^{-1/2}$\n",
    "4. Compute the first $k$ eigenvectors of $L$. Let $V = \\left( v_1, \\ldots, v_k \\right)$ where each column is one of the eigenvectors.\n",
    "5. With each row of $V$ corresponding to one of the data points in the original data space, cluster the rows with k0means algorithm to get cluster assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to Construct Similarity Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In more detail, there are multiple ways to build the similarity graph from raw data points. First of all, common distance metrics like Pearson correlation or cosine similarity can all be used to define similarity between data points, after which many different techiniques can be used to build a similarity graph. Some of the most common ones are explained below.\n",
    "\n",
    "- The simplest way is to just use the similarity matrix as the adjacency matrix of the graph. This will result in a **fully connected weighted graph**.\n",
    "- The second way is to set a threshold $\\epsilon$. Only the pairs of points with similarity above $\\epsilon$ will be connected, with the similarities as the weights of the edges connecting the pairs. This is some times referred to as **$\\epsilon$-neighborhood graph**, or asset graph.\n",
    "- **$k$-nn** is another method to define the graph, where poing $i$ is connected to piont $j$ if $i$ is one of the $k$ nearest neighbors of $j$.\n",
    "    - This will usually result in a directed graph since the neighborhood relationship is not symmetric. The common spectral clustering algorithms only apply to undirected graphs, but there are directed versions of this algorithm, too.\n",
    "    - We can also get a undirected similarity graph, either by just ignoring the direction, or only connnecting the nodes who are mutual $k$ nearesst neighbors.\n",
    "- We can also start from the fully connected graph and then retrieve the **minimum spanning tree (MST)** from it. While the MST is expected to capture the most important structure from the graph, sometimes it looses too much information. Other methods like **planar maximally filtered graphs (PMFG)** looks for subgraphs that at least includes all edges in the MST.\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm described above, as well as many other more complicated variations of this algorithm, needs to compute the eigenvalues and eigenvectors of a potentially large matrix. Here we list some of the most common algorithms.\n",
    "\n",
    "- **Power iteration**: gives the eigenvector corresponding to the largest eigenvalue. This can be easily implementd in parallel, but the convergence depends on the ratio $\\lambda_2 / \\labmda_1$.\n",
    "- **QR Iteration**: gives all eigenvalues and eigenvectors of A, but the QR facotrization on updated matrices is sequential.\n",
    "- **Lanczos/Arnoldi Iteration**: basically it produces a upper Hessenburg matrix column by column using _only matrix multiplication_. Then the eigenvalues of the result Hessenburg matrix as approximation. The Fortran package ARPACK implements this algorithm and Spark provides access to this in the MLlib module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a really basic introduction to some of the Spark (PySpark) functionalities. To use Spark with Jupyter Notebook, one has to either configure Spark to start the Notebook every time it launches, or tell the notebook server where to find Spark at the beginning of the document. Here we use the second approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:00:43.635882Z",
     "start_time": "2019-03-09T08:00:43.628215Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to create a Spark Context before importing anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:00:51.653722Z",
     "start_time": "2019-03-09T08:00:46.197204Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='myApp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark supports read and writing in many different file formats on the local file system. (Code below is **copied directly** from the Learning Spark book in the reference!)\n",
    "\n",
    "- Text file\n",
    "    ```python\n",
    "    input = sc.textFile(\"file:///home/holden/repos/spark/README.md\")\n",
    "    ```\n",
    "- JSON\n",
    "    ```python\n",
    "    import json\n",
    "    data = input.map(lambda x: json.loads(x))\n",
    "    ```\n",
    "- Reading CSV\n",
    "    ```python\n",
    "    import csv\n",
    "    import StringIO\n",
    "    \n",
    "    def loadRecord(line):\n",
    "        \"\"\"Parse a CSV line\"\"\"\n",
    "        input = StringIO.StringIO(line)\n",
    "        reader = csv.DictReader(input, fieldnames=[\"name\", \"favouriteAnimal\"])\n",
    "        return reader.next()\n",
    "    input = sc.textFile(inputFile).map(loadRecord)\n",
    "    ```\n",
    "- Writing to CSV\n",
    "    ```python\n",
    "    def writeRecords(records):\n",
    "        \"\"\"Write out CSV lines\"\"\"\n",
    "        output = StringIO.StringIO()\n",
    "        writer = csv.DictWriter(output, fieldnames=[\"name\", \"favoriteAnimal\"])\n",
    "        for record in records:\n",
    "            writer.writerow(record)\n",
    "        return [output.getvalue()]\n",
    "    \n",
    "    ```\n",
    "- Other file formats (like Parquet) are supported as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the local file system, Spark also incorporates other popular file systems like Amazon S3 and Hadoop Distributed File System. Also, there is the Spark SQL module that uses SQL query for retrieving data. The query results are in a general format and can be casted to common types using methods like getFloat(), getInt(), getString(), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD stands for **Resilient Distributed Dataset**, which is the basic data structure that allows Spark to handle data the way it does. Recently Spark has been migrating from the basci RDD to a more \"modern\" data structure called dataframe that is expected to provide more user-friendly interfaces. Despite being promising, this upgrade is still in progress and in this tutorial we will focus more on RDD.\n",
    "\n",
    "For the code below to run, we will need a subdirectory named \"data\" under the working directory and a .txt file called README.txt inside, which is just some random text used for demonstration.\n",
    "\n",
    "Now here is how to read lines from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:00:56.971535Z",
     "start_time": "2019-03-09T08:00:54.546323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "# Find spark\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile('data/README.md') # the README file from findspark project page on Github\n",
    "print(lines.count())\n",
    "print(lines.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important feature of Spark is **lazy evaluation**, which means that Spark won't actually evaluate anything until when we need the results. So the code below just **transforms** a RDD into another RDD but does not evaluate what is actualy inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:00:58.140166Z",
     "start_time": "2019-03-09T08:00:58.105735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparklines = lines.filter(lambda line: \"spark\" in line)\n",
    "sparklines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we need the values, e.g. first 3 items, then Spark will have compute the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:01:05.091550Z",
     "start_time": "2019-03-09T08:01:05.016354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Find spark',\n",
       " 'You can address this by either symlinking pyspark into your site-packages,',\n",
       " 'or adding pyspark to sys.path at runtime. `findspark` does the latter.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparklines.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the methods like `filter` are **transformations** while those like `take` are **actions**.\n",
    "\n",
    "- **transformations** return RDDs and are exectued lazily\n",
    "    - filter()\n",
    "    - union(), distinct(), intersetion(), suctract(), cartesian(), ...\n",
    "    - map(), flatMap()\n",
    "    \n",
    "- **actions** return other types of data and kick off computations\n",
    "    - count()\n",
    "    - take()\n",
    "    - reduce(), fold()\n",
    "    - aggregate()\n",
    "    - ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lazy execution of transformations helps to avoid unecessary (and potentially costly) computation in most cass. But when some of the values are repeatedly used in our programs, we can let Spark \"remember\" the results by calling `persist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:01:07.497768Z",
     "start_time": "2019-03-09T08:01:07.470816Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pythonlines = lines.filter(lambda line: \"Python\" in line).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the methods shown above we can perform most data transformation and aggregation we want. But it still helps to go one step further by storing data in key-value pairs, which is what paried RDD means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:01:08.973828Z",
     "start_time": "2019-03-09T08:01:08.862512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', '# Find spark'),\n",
       " ('', ''),\n",
       " ('PySpark',\n",
       "  \"PySpark isn't on sys.path by default, but that doesn't mean it can't be used as a regular library.\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_pairs = lines.map(lambda ll: (ll.split(' ')[0], ll))\n",
    "line_pairs.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that paired RDD is just key-value pairs stored in regular RDDs. However, Spark also provides some handy methods to operate on key-value pairs. For example, if I want to predict the number of words each line from the first word of the line (alright I just don't want to think of a better example), it might be useful to see the average number of words grouped by the first word of each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:01:12.139628Z",
     "start_time": "2019-03-09T08:01:11.474976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', (3, 1)), ('', (23, 17)), ('PySpark', (18, 1)), ('```python', (4, 4))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reducing by key (the first word), use a tuple to get sum of words and number of lines\n",
    "line_pairs = lines.map(lambda ll: (ll.split(' ')[0], (len(ll.split(' ')), 1)))\n",
    "line_counts = line_pairs.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "line_counts.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:01:13.074155Z",
     "start_time": "2019-03-09T08:01:12.997879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 3.0),\n",
       " ('', 1.3529411764705883),\n",
       " ('PySpark', 18.0),\n",
       " ('```python', 1.0),\n",
       " ('import', 2.0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Use another map for average number per line.\n",
    "line_counts.map(lambda x: (x[0], x[1][0] / x[1][1])).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from these methods just mentioned, paired RDD also has\n",
    "- other transformations including joins, sorting, etc.\n",
    "- other actions include:\n",
    "    - countByKey()\n",
    "    - collectAsMap()\n",
    "    - lookup()\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark ML and MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has a module for machine learning called MLlib, where you can find the most common machine learning algorithms, as well as some data structures built on top of RDD, and other utilities. There is also a `spark.ml` module that is based on dataframes. However, since we are interested in implementing spectral clustering with Spark, here we only focus on those related to linear algebra.\n",
    "\n",
    "Here are some data structures from `pyspark.mllib.linalg.distributed`, which are essentially RDDs with more methods and attributes that help us do linear algebra:\n",
    "\n",
    "- **CoordinateMatrix**\n",
    "    - Tuples of (row number, column number, value) representing the entries in a matrix.\n",
    "    - It is really useful for creating a distributed matrix from other data types (like lines from a .txt file) and then converting it to another type of matrix to actually do linear algebra. In fact, it only has 6 methods. Two gets the number of rows or columns, one transposes the matrix, and the rest just converts the matrix to other forms.\n",
    "- **RowMatrix**\n",
    "    - A row-oriented (distributed) matrix that actually has some useful methods.\n",
    "    - Can get column similarity and other summary statistics.\n",
    "    - Can do SVD and PCA. The former is really useful for our purpose, while the latter sucks since it has an upper limit on the number of columns (no more than 65,535).\n",
    "- **IndexedRowMatrix**\n",
    "    - Just like RowMatrix but with indexed rows, which might be quite useful in some cases.\n",
    "- **BlockMatrix**\n",
    "    - A distributed matrix in blocks of _local_ matrices.\n",
    "    - This really useful for adding, subtracting, and multiplying matrices.\n",
    "- ...\n",
    "   \n",
    "So when we actually try to use these classes to do linear algebra, we often have to use a lot of `toRowMatrix` or `toBlockMatrix` before actually doing anything. But this usually does not cost very much if we make sure that all the data structure involved are **all distributed matrices**.\n",
    "\n",
    "In the next section I will use these to do spectral clustering on a network of emails between Europe institutions. You can see that they are actually **quite helpful** when used properly. (And pretty slow when I throw in dataframe, but I will do it anyway so that I don't have to write my own k-means.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying Applications with spark-submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `spark-submit` to run our programs. The format is\n",
    "\n",
    "```bash\n",
    "bin/spark-submit [options] <app jar | python script> [app options]\n",
    "```\n",
    "\n",
    "We can specify multiple different options, like whether to deploy locally, which cluster manager to use, how much memory is available for the worker nodes, and so on. For example, \n",
    "\n",
    "```shell\n",
    "bin/spark-submit --master spark://host:7707 --executor-memory 10g my_script.py\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```shell\n",
    "bin/spark-submit --class org.apache.spark.examples.SparkPi \\\n",
    "    --master yarn \\\n",
    "    --deploy-mode cluster \\\n",
    "    --driver-memory 4g \\\n",
    "    --executor-memory 2g \\\n",
    "    --executor-cores 1 \\\n",
    "    --queue thequeue \\\n",
    "    examples/jars/spark-examples*.jar \\\n",
    "    10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Toy Example: Spectral Clustering on An Email Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:01:16.622141Z",
     "start_time": "2019-03-09T08:01:16.348563Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession(sc)\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix, MatrixEntry, CoordinateMatrix\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:05:42.326196Z",
     "start_time": "2019-03-09T08:05:42.280411Z"
    }
   },
   "outputs": [],
   "source": [
    "## Read data, get random sample so the memory won't explode when running locally.\n",
    "txt = sc.textFile('./data/email-Eu-core.txt')\n",
    "txt = txt.map(lambda x: x.split(' ')).map(lambda x: (int(x[0]) ,int(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:05:43.429734Z",
     "start_time": "2019-03-09T08:05:43.241720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Calculate number of nodes, which is required for constructing CoordinateMatrix\n",
    "N = txt.flatMap(lambda x: [int(xx) for xx in x]).max() + 1\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:05:45.777376Z",
     "start_time": "2019-03-09T08:05:44.928374Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Created as CoordinateMatrix.\n",
    "upper_entries = txt.map(lambda x: MatrixEntry(int(x[0]), int(x[1]), 1.0))\n",
    "lower_entries = txt.map(lambda x: MatrixEntry(int(x[1]), int(x[0]), 1.0))\n",
    "degrees = upper_entries.map(lambda entry: (entry.i, entry.value)).reduceByKey(lambda a, b: a + b)\n",
    "W = CoordinateMatrix(upper_entries.union(lower_entries), numCols=N, numRows=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:05:49.491780Z",
     "start_time": "2019-03-09T08:05:46.141246Z"
    }
   },
   "outputs": [],
   "source": [
    "## Graph Laplacian.\n",
    "## Converting between BlockMatrix and CoordinateMatrix to access those methods.\n",
    "entries = degrees.map(lambda x: MatrixEntry(x[0], x[0], 1/x[1]))\n",
    "D_inv = CoordinateMatrix(entries, numCols=N, numRows=N).toBlockMatrix()\n",
    "I = CoordinateMatrix(sc.range(N).map(lambda i: MatrixEntry(i, i, 1.0)), numCols=N, numRows=N).toBlockMatrix()\n",
    "L = I.subtract(D_inv.multiply(W.toBlockMatrix())).toCoordinateMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:05:56.505776Z",
     "start_time": "2019-03-09T08:05:50.392844Z"
    }
   },
   "outputs": [],
   "source": [
    "## SVD. Have to convert to RowMatrix.\n",
    "K = 2\n",
    "svd = L.toRowMatrix().computeSVD(k=K, computeU=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:06:02.249673Z",
     "start_time": "2019-03-09T08:05:57.215379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=0.09460881494192298, _2=0.09253012645130627, features=DenseVector([0.0946, 0.0925]), prediction=0),\n",
       " Row(_1=0.08248300664640791, _2=0.09441851290327175, features=DenseVector([0.0825, 0.0944]), prediction=0),\n",
       " Row(_1=0.0018786162281686377, _2=-0.004914748023745792, features=DenseVector([0.0019, -0.0049]), prediction=0),\n",
       " Row(_1=0.0014804572625659344, _2=-0.004008069326282159, features=DenseVector([0.0015, -0.004]), prediction=0),\n",
       " Row(_1=0.007045888350176693, _2=-0.011045064226871897, features=DenseVector([0.007, -0.011]), prediction=0),\n",
       " Row(_1=0.006127257232272677, _2=-0.014279414284443464, features=DenseVector([0.0061, -0.0143]), prediction=0),\n",
       " Row(_1=0.0028771678050424424, _2=-0.004467594100664339, features=DenseVector([0.0029, -0.0045]), prediction=0),\n",
       " Row(_1=0.016275353005051955, _2=-0.012842797617696335, features=DenseVector([0.0163, -0.0128]), prediction=0),\n",
       " Row(_1=0.000407637951511903, _2=-0.00014602404104216416, features=DenseVector([0.0004, -0.0001]), prediction=0),\n",
       " Row(_1=0.000936186541510611, _2=-8.168415200530293e-05, features=DenseVector([0.0009, -0.0001]), prediction=0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is where I start to convert things first to lists then to dataframes.\n",
    "V = svd.V.toArray().tolist()\n",
    "VV = spark.createDataFrame(V)\n",
    "vecAssembler = VectorAssembler(inputCols=VV.schema.names, outputCol='features')\n",
    "VV = vecAssembler.transform(VV)\n",
    "\n",
    "## Kmeans from ml, not mllib.\n",
    "kmeans = KMeans().setK(K).setSeed(1)\n",
    "model = kmeans.fit(VV.select('features'))\n",
    "clusters = model.transform(VV)\n",
    "clusters.head(10)\n",
    "\n",
    "## Save results to local pandas data frames.\n",
    "## Not doing it here.\n",
    "## clusters.toPandas().to_csv('./out/assignment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:06:08.224744Z",
     "start_time": "2019-03-09T08:06:07.649830Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Always stop spark context in the end.\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- Von Luxburg, U. (2007). A tutorial on spectral clustering. Statistics and computing, 17(4), 395-416.\n",
    "- [Karau, Holden, Andy Konwinski, Patrick Wendell, and Matei Zaharia. Learning spark: lightning-fast big data analysis. \" O'Reilly Media, Inc.\", 2015.](https://proquest-safaribooksonline-com.proxy.library.cmu.edu/book/databases/business-intelligence/9781449359034/preface/idp4948496_html#X2ludGVybmFsX0h0bWxWaWV3P3htbGlkPTk3ODE0NDkzNTkwMzQlMkZjaGFwX3BhaXJfcmRkc19odG1sJnF1ZXJ5PQ==)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
